type: paper
iaifi-thrust: F
arxiv-date: 2025-05-04
title: "Heterosynaptic Circuits Are Universal Gradient Machines"
authors: "Liu Ziyin, Isaac Chuang, Tomaso Poggio"
abstract: "We propose a design principle for the learning circuits of the biological brain. The principle states that almost any dendritic weights updated via heterosynaptic plasticity can implement a generalized and efficient class of gradient-based meta-learning. The theory suggests that a broad class of biologically plausible learning algorithms, together with the standard machine learning optimizers, can be grounded in heterosynaptic circuit motifs. This principle suggests that the phenomenology of (anti-) Hebbian (HBP) and heterosynaptic plasticity (HSP) may emerge from the same underlying dynamics, thus providing a unifying explanation. It also suggests an alternative perspective of neuroplasticity, where HSP is promoted to the primary learning and memory mechanism, and HBP is an emergent byproduct. We present simulations that show that (a) HSP can explain the metaplasticity of neurons, (b) HSP can explain the flexibility of the biology circuits, and (c) gradient learning can arise quickly from simple evolutionary dynamics that do not compute any explicit gradient. While our primary focus is on biology, the principle also implies a new approach to designing AI training algorithms and physically learnable AI hardware. Conceptually, our result demonstrates that contrary to the common belief, gradient computation may be extremely easy and common in nature."
arxiv: "2505.02248"
journal:
doi:
nsf-par:
code:
publication-date:
bib-tex: |
  @article{ziyin2025heterosynapticcircuitsuniversalgradient,
      title={Heterosynaptic Circuits Are Universal Gradient Machines},
      author={Liu Ziyin and Isaac Chuang and Tomaso Poggio},
      year={2025},
      eprint={2505.02248},
      archivePrefix={arXiv},
      primaryClass={q-bio.NC},
      url={https://arxiv.org/abs/2505.02248},
  }
