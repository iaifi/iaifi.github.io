type: paper
iaifi-thrust: F
arxiv-date: 2025-01-23
title: "SIDDA: SInkhorn Dynamic Domain Adaptation for Image Classification with Equivariant Neural Networks"
authors: "Sneh Pandya, Purvik Patel, Brian D. Nord, Mike Walmsley, Aleksandra Ćiprijanović"
abstract: "Modern neural networks (NNs) often do not generalize well in the presence of a 'covariate shift'; that is, in situations where the training and test data distributions differ, but the conditional distribution of classification labels remains unchanged. In such cases, NN generalization can be reduced to a problem of learning more domain-invariant features. Domain adaptation (DA) methods include a range of techniques aimed at achieving this; however, these methods have struggled with the need for extensive hyperparameter tuning, which then incurs significant computational costs. In this work, we introduce SIDDA, an out-of-the-box DA training algorithm built upon the Sinkhorn divergence, that can achieve effective domain alignment with minimal hyperparameter tuning and computational overhead. We demonstrate the efficacy of our method on multiple simulated and real datasets of varying complexity, including simple shapes, handwritten digits, and real astronomical observations. SIDDA is compatible with a variety of NN architectures, and it works particularly well in improving classification accuracy and model calibration when paired with equivariant neural networks (ENNs). We find that SIDDA enhances the generalization capabilities of NNs, achieving up to a ≈40% improvement in classification accuracy on unlabeled target data. We also study the efficacy of DA on ENNs with respect to the varying group orders of the dihedral group DN, and find that the model performance improves as the degree of equivariance increases. Finally, we find that SIDDA enhances model calibration on both source and target data--achieving over an order of magnitude improvement in the ECE and Brier score. SIDDA's versatility, combined with its automated approach to domain alignment, has the potential to advance multi-dataset studies by enabling the development of highly generalizable models."
arxiv: "2501.14048"
journal: "Machine Learning: Science and Technology, 2025, Volume 6, Number 3"
doi: https://doi.org/10.1088/2632-2153/adf701
nsf-par:
code: https://github.com/deepskies/SIDDA
publication-date: 2025-08-14
bib-tex: |
  @article{Pandya_2025,
  doi = {10.1088/2632-2153/adf701},
  url = {https://doi.org/10.1088/2632-2153/adf701},
  year = {2025},
  month = {aug},
  publisher = {IOP Publishing},
  volume = {6},
  number = {3},
  pages = {035032},
  author = {Pandya, Sneh and Patel, Purvik and Nord, Brian D and Walmsley, Mike and ƒÜiprijanoviƒá, Aleksandra},
  title = {SIDDA: SInkhorn Dynamic Domain Adaptation for image classification with equivariant neural networks},
  journal = {Machine Learning: Science and Technology},
  abstract = {Modern neural networks (NNs) often do not generalize well in the presence of a ‚Äòcovariate shift‚Äô; that is, in situations where the training and test data distributions differ, but the conditional distribution of classification labels given the data remains unchanged. In such cases, NN generalization can be reduced to a problem of learning more robust, domain-invariant features. Domain adaptation (DA) methods include a broad range of techniques aimed at achieving this; however, these methods have struggled with the need for extensive hyperparameter tuning, which then incurs significant computational costs. In this work, we introduce SInkhorn Dynamic Domain Adaptation (SIDDA), an out-of-the-box DA training algorithm built upon the Sinkhorn divergence, that can achieve effective domain alignment with minimal hyperparameter tuning and computational overhead. We demonstrate the efficacy of our method on multiple simulated and real datasets of varying complexity, including simple shapes, handwritten digits, real astronomical observations, and remote sensing data. These datasets exhibit covariate shifts due to noise, blurring, differences between telescopes, and variations in imaging wavelengths. SIDDA is compatible with a variety of NN architectures, and it works particularly well in improving classification accuracy and model calibration when paired with symmetry-aware equivariant NNs (ENNs). We find that SIDDA consistently enhances the generalization capabilities of NNs, achieving up to a  improvement in classification accuracy on unlabeled target data, while also providing a more modest performance gain of  on labeled source data. We also study the efficacy of DA on ENNs with respect to the varying group orders of the dihedral group DN, and find that the model performance improves as the degree of equivariance increases. Finally, if SIDDA achieves proper domain alignment, it also enhances model calibration on both source and target data, with the most significant gains in the unlabeled target domain‚Äîachieving over an order of magnitude improvement in the expected calibration error and Brier score. SIDDA‚Äôs versatility across various NN models and datasets, combined with its automated approach to domain alignment, has the potential to significantly advance multi-dataset studies by enabling the development of highly generalizable models.},
  eprint={2501.14048},
  archivePrefix={arXiv},
  }
