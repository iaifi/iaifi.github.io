type: paper
iaifi-thrust: F
arxiv-date: 2025-07-01
title: "Proof of a perfect platonic representation hypothesis"
authors: "Liu Ziyin, Isaac Chuang"
abstract: "In this note, we elaborate on and explain in detail the proof given by Ziyin et al. (2025) of the 'perfect' Platonic Representation Hypothesis (PRH) for the embedded deep linear network model (EDLN). We show that if trained with SGD, two EDLNs with different widths and depths and trained on different data will become Perfectly Platonic, meaning that every possible pair of layers will learn the same representation up to a rotation. Because most of the global minima of the loss function are not Platonic, that SGD only finds the perfectly Platonic solution is rather extraordinary. The proof also suggests at least six ways the PRH can be broken. We also show that in the EDLN model, the emergence of the Platonic representations is due to the same reason as the emergence of progressive sharpening. This implies that these two seemingly unrelated phenomena in deep learning can, surprisingly, have a common cause. Overall, the theory and proof highlight the importance of understanding emergent 'entropic forces' due to the irreversibility of SGD training and their role in representation learning. The goal of this note is to be instructive and avoid lengthy technical details"
arxiv: "2507.01098"
journal:
doi:
nsf-par:
code:
publication-date:
bib-tex: |
  @article{ziyin2025proofperfectplatonicrepresentation,
      title={Proof of a perfect platonic representation hypothesis},
      author={Liu Ziyin and Isaac Chuang},
      year={2025},
      eprint={2507.01098},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2507.01098},
  }
