type: paper
iaifi-thrust: F
title: "The Quantization Model of Neural Scaling"
authors: Eric J. Michaud, Ziming Liu, Uzay Girit, Max Tegmark
abstract: 'We propose the Quantization Model of neural scaling laws, explaining both the observed power law dropoff of loss with model and data size, and also the sudden emergence of new capabilities with scale. We derive this model from what we call the Quantization Hypothesis, where learned network capabilities are quantized into discrete chunks (quanta). We show that when quanta are learned in order of decreasing use frequency, then a power law in use frequencies explains observed power law scaling of loss. We validate this prediction on toy datasets, then study how scaling curves decompose for large language models. Using language model internals, we auto-discover diverse model capabilities (quanta) and find tentative evidence that the distribution over corresponding subproblems in the prediction of natural text is compatible with the power law predicted from the neural scaling exponent as predicted from our theory.'
arxiv: "2303.13506"
journal: 
doi: 
nsf-par:
code: https://github.com/ejmichaud/quantization-model
publication-date: 
bib-tex: |
  @misc{michaud2024quantizationmodelneuralscaling,
      title={The Quantization Model of Neural Scaling},
      author={Eric J. Michaud and Ziming Liu and Uzay Girit and Max Tegmark},
      year={2024},
      eprint={2303.13506},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.13506},
  }
