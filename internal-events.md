---
layout: article
title: Internal Events
---

* [Upcoming Internal Discussion Seminars](#upcoming-internal-discussion-seminars)
* [Upcoming Journal Clubs](#upcoming-journal-clubs)
* [Other Upcoming Internal Events](#other-upcoming-internal-events)
* [Past Seminars](#past-seminars)
* [Past Journal Clubs](#past-journal-clubs)
* [Other Past Internal Events](#other-past-internal-events)

<style>
.calendar-container{
    position: relative;
    Padding-bottom: 75%;
    Height: 0;
    overflow: hidden;
}
</style>

<style>
.calendar-container iframe{
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
}
</style>

<div class="calendar-container">
<iframe src="https://calendar.google.com/calendar/embed?height=300&wkst=1&bgcolor=%23ffffff&ctz=America%2FNew_York&showNav=1&showPrint=0&showCalendars=0&mode=AGENDA&src=cDcxb2tybHAxZWJvazFpMjdtc2gzZm9kdThAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&src=ZGFicWU3OXZxY3NoMTFkMjdsY2Q5OGxlbXNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&color=%238E24AA&color=%23D50000" style="border:solid 1px #777" width="800" height="300" frameborder="0" scrolling="no"></iframe>
</div>

**Note: Events in red are invite-only**

To add this calendar to your Google Calendars click the +GoogleCalendar button in the bottom right corner of the calendar. To add an individual event to your calendar, click on the event and choose "copy to my calendar."

[Click here to add this calendar to a different calendar application](https://calendar.google.com/calendar/ical/p71okrlp1ebok1i27msh3fodu8%40group.calendar.google.com/public/basic.ics)

## Upcoming Internal Discussion Seminars
Unless otherwise noted, discussion seminars will be held in person (MIT Kolker Room, Building 26, Room 414) and over [Zoom](https://www.google.com/url?q=https://mit.zoom.us/j/92183041364?pwd%3DN3pMelhpV3JUOVkzcjl1cTR4UVd6Zz09&sa=D&source=calendar&usd=2&usg=AOvVaw0SMrjNzSOUddjpaY3nOnCC). 

* **Kaća Bradonjić, Professor, Hampshire College**
    * **Friday, October 21, 2022, 2:00–3:00 pm**
    * *Return to the phenomenal: An exploration of the subjective, internal representations of physical theories and their relation to the collective pursuits of knowledge* 
    * Physicists study the physical world on spatial, temporal and complexity scales inaccessible through ordinary human perception. *How, then, does a person ground their understanding of physics at these scales in the sensory impressions and emotional states made possible by their body?* In relation to the field as a whole, we can also ask: *To what extent and how does a physicist’s research methodology (theoretical, experimental, or computational) shape the features of the mental models they use in their work? Conversely, to what extent and how do a physicist’s mental models affect the way they approach and engage with a research problem? Finally, what is the nature of the dynamic relation between the individual mental models and their collectively-accepted representational counterparts, and how does it impact physics research?* In this talk, I will sketch out the framing of my approach to these questions that integrates artistic and intellectual practices, and is informed by the history and philosophy of science, theories of embodied cognition, and philosophy of phenomenology. I will then describe the exploratory stages of my first project in this vein, carried out at IAIFI, with the particular focus on the role of AI in particle physics research. 

* **Abiy Tasissa, Professor, Tufts**
    * **Friday, October 28, 2022 2:00-3:00pm** 
    * *Geometric sparse coding with learned archetypes: Theory and applications*
    * Given a set of data points, archetypal analysis is a method which represents each data as a convex combination of exemplars called "archetypes". The benefit of this analysis is the interpretable archetypes along with information that can be gleaned from the representation coefficients. We propose a method that combines manifold learning and archetypal analysis by positing that each data point can be written as a convex combination of nearby landmarks. To encourage representing a data point via closeby landmarks, we propose a locality regularizer. We discuss how this regularizer relates to graph matching, K-means and Laplacian smoothness. Under the assumption that the data is exactly generated from vertices of a Delaunay triangulation,the proposed regularizer exactly recovers the underlying sparse solution. Moreover, for fixedrepresentation coefficients, we show that the optimal landmarks can be computed in closed form. To solve the optimization problem of finding the coefficients and the landmarks, we use algorithm unrolling to derive a neural network that efficiently solves the problem. We discuss how the sparseembeddings derived from our algorithm can be used for downstream tasks such as clustering. 

* **Aleksander Madry, Professor, MIT**
    * **Friday, December 9, 2022 2:00-3:00pm** 
    * *Details to come*

## Upcoming Journal Clubs
The IAIFI Journal Club is only open to IAIFI members and affiliates. Access to the Zoom information and recordings can be found on the IAIFI internal website (contact [iaifi@mit.edu](mailto:iaifi@mit.edu) if you have trouble logging in).

[Sign up to lead a discussion!](https://forms.gle/zfpT4QQdXg8tu6VB7)

* **Sona Najafi, Researcher, IBM**
  * **October 25, 2022, 11:00am-12:00pm**
  * *Quantum machine learning from algorithms to hardware*
  * Abstract: The rapid progress of technology over the past few decades has led to the emergence of two powerful computational paradigms known as quantum computing and machine learning. While machine learning tries to learn the solutions from data, quantum computing harnesses the quantum laws for more powerful computation compared to classical computers. In this talk, I will discuss three domains of quantum machine learning, each harnessing a particular aspect of quantum computers and targeting specific problems. The first domain scrutinizes the power of quantum computers to work with high-dimensional data and speed-up algebra, but raises the caveat of input/output due to the quantum measurement rules. The second domain circumvents this problem by using a hybrid architecture, performing optimization on a classical computer while evaluating parameterized states on a quantum circuit, chosen based on a particular issue. Finally, the third domain is inspired by brain-like computation and uses a given quantum system's natural interaction and unitary dynamic as a source for learning

## Other Upcoming Internal Events
Internal events are only open to IAIFI members and affiliates. Access to the Zoom information and recordings can be found on the IAIFI internal website (contact [iaifi-management@mit.edu](mailto:iaifi-management@mit.edu) if you have trouble logging in).

For Fall 2022, IAIFI will hold regular lightning talk sessions for IAIFI members to share their research along specific themes. 

* Lightning Talk Session I 
    * **September 23, 2022, 2:00pm-3:00pm**
    * Andrew Saydjari (Grad Student, Harvard), "Marginalized Data-space Gaussian Inference for Component Separation (MaDGICS): An Application to Stellar Spectra"
    * Harold Erbin (Postdoc, MIT), "Neural networks for string field theory"
    * Zev Imani (Grad Student, Tufts), "Score-Based Generative Modeling"

* Special Seminar: David Stork
    * **September 23, 2022, 3:00pm–4:00pm** (following lightning talks)
    * *When computers look at art*

* Industry Lunch Seminar: QuERa
    * **October 19, 2022, 12:00pm–1:00pm** (Lunch will be served)
    * Pedro Lopes, Fangli Liu, Shengtao Wang, Tommaso Macri
    * QuEra Computing is shaping the future of information systems by developing advanced quantum computing hardware, software, and applications. Leveraging a scalable and flexible platform using neutral-atom qubits, our systems enable customers and partners to explore the power of quantum processors with hundreds of qubits. With a focus on utility, we emphasize modes of operation and hardware-efficient algorithms that make the most efficient use of the hardware capabilities. Our processors empower users to make scientific breakthroughs, as well as test the limits of quantum vs classical solutions for business-oriented applications. Our first device operates 256 qubits in analog mode. From a scientific discovery perspective these capacities mean unparalleled opportunities to study time-dependent quantum phenomena at the limit of classical computing (and potentially beyond). Problems within range are not fully universal, but include phase transitions and quenches, lattice gauge theories, particle collisions, and more. Our computers also serve as large reservoirs of quantum complexity, which can be tapped for applications in machine learning. With this introductory talk, we intend to open conversations with the IAIFI and local lattice gauge theory communities. We seek to stimulate interest in developing joint applications to quantum machine learning and high-energy physics.

* Lightning Talk Session II 
    * **November 18, 2022, 2:00pm-3:00pm**
    * *Theme details to come*

* Lightning Talk Session II 
    * **December 16, 2022, 2:00pm-3:00pm**
    * *Theme details to come*

## Past Seminars
### Fall 2022
* **Jessie Micallef, IAIFI Fellow**
    * **Friday, September 30, 2022 2:00-3:00pm** 
    * *Neutrinos and Neural Networks: Need for Speed and Adaptability*
    * Abstract: Neutrinos remain an elusive and intriguing fundamental particle that is useful for probing inconsistencies of the Standard Model: neutrinos have mass when the Standard Model predicts they should not, they potentially exhibit charge parity violation, and there are possible hints of a fourth, sterile neutrino flavor. Data from neutrino detectors is particularly valuable due to the neutrinos’ weakly interacting nature, thus it is crucial that we maximize the information per detected interaction. In this talk, I will show how we are using machine learning to better analyze the precious data from various types of neutrino detectors. I will discuss optimizing convolutional neural networks (CNNs) to reconstruct GeV-scale neutrino events in the IceCube detector and how these measurements can help improve our understanding of these difficult-to-detect particles. I will focus on the challenges of reconstructing sparse, noisy neutrino events along with the speedup advantages of using machine learning methods. I will also touch on challenges that machine learning reconstructions face with the current and next generation of neutrino experiments, which will leverage Liquid Argon (LAr) detectors that use charge and light to record neutrino interactions.

### Spring 2022
* **Anna Golubeva, IAIFI Fellow**
    * **Friday, March 11, 2022 2:00-3:00pm** 
    * *"The role of symmetry in machine learning"*
    * Abstract: In physics, symmetry is a concept of fundamental importance. It has served as a powerful guiding principle that allows us to find regularities in complex phenomena and to deduce the underlying simple laws of nature. Can we leverage the principle of symmetry to gain insights into Machine Learning? There are three separate but interconnected parts of a ML system where we could look for symmetries: The neural network architecture, the input data and the loss function. I will give an overview of the existing research on this topic and discuss the implications for practical ML.

* **Boaz Borak, Professor, Computer Science, Harvard**
    * **Friday, April 8, 2022 2:00-3:00pm** 
    * *Deep learning, generalization, and rationality*
    * Abstract: Deep learning often operates in a regime where traditional generalization bounds fail to hold, and indeed are not even true, in the sense that there is a non vanishing gap between empirical and population performance. Yet, deep neural networks still generalize and perform well beyond their training set.  In this talk we will present: (1) Empirical evidence that deep networks have similar internal representations regardless of whether they are trained in the traditional "full supervised" manner or trained in a "self supervised + simple" (SSS) method, where all but their last layer are trained without access to the labels; (2)  Empirical evidence that for SSS algorithms, generalization is true in practice, along with a theoretical bound on the generalization gap of such algorithms which is non vacuous in several practical setting. The bound does not make structural or conditional independence assumptions on the training distribution, but rather assumes the algorithm is "rational" in a certain precise sense, which is empirically shown to hold in practice.
The talk will not require background in deep learning. Based on joint works with Yamini Bansal, Gal Kaplun, and Preetum Nakkiran.

* **Siddharth Mishra-Sharma, IAIFI Fellow**
    * **Friday, April 22, 2022 4:00-5:00pm** 
    * *Flows for inference and interpretability: a Galactic Center Excess case study*
    * Abstract: The source of the so-called Galactic Center Excess (GCE)---an excess of gamma-rays observed from the central regions of the Milky Way---remains an open question. Disentangling the various possibilities, such as annihilating dark matter and astrophysical point sources, is a challenging modeling and inference task. I will describe some recent attempts at making progress in this direction by leveraging neural simulation-based inference techniques. Time permitting, I will describe some ongoing work using generative modeling as a test of robustness of neural network-based inference methods in the context of the GCE.

* **Special Seminar: Junyu Liu**
  * **Wednesday, June 1, 2022 2:30-3:30pm** 
  * *An analytic theory for the dynamics of wide quantum neural networks*
  * Abstract: Parametrized quantum circuits can be used as quantum neural networks and have the potential to outperform their classical counterparts when trained for addressing learning problems. To date, much of the results on their performance on practical problems are heuristic in nature. In particular, the convergence rate for the training of quantum neural networks is not fully understood. Here, we analyze the dynamics of gradient descent for the training error of a class of variational quantum machine learning models. We define wide quantum neural networks as parameterized quantum circuits in the limit of a large number of qubits and variational parameters. We then find a simple analytic formula that captures the average behavior of their loss function and discuss the consequences of our findings. For example, for random quantum circuits, we predict and characterize an exponential decay of the residual training error as a function of the parameters of the system. We finally validate our analytic results with numerical experiments.


### Fall 2021

  * **Fabian Ruehle, Assistant Professor, Northeastern University**
    * **Friday, September 24, 2:00-3:00pm** 
    * *"Learning metrics in extra dimensions"*
    * Abstract: String theory is a very promising candidate for a fundamental theory of our universe. An interesting prediction of string theory is that spacetime is ten-dimensional. Since we only observe four spacetime dimensions, the extra six dimensions are small and compact, thus evading detection. These extra six-dimensional spaces, known as Calabi-Yau spaces, are very special and elusive. They are equipped with a special metric needed to make string theory consistent. This special property is given in terms of a (notoriously hard) type of partial differential equation. While we know, thanks to the heroic work of Calabi and Yau, that this PDE has a unique solution and hence that the metric exists, we neither know what it looks like nor how to construct it explicitly. However, the metric is an important quantity that enters in many physical observables, e.g. particle masses. Thinking of the metric as a function that satisfies three constraints that enter in the Calabi-Yau theorem, we can parameterize the metric as a neural network and formulate the problem as multiple continuous optimization tasks. The neural network is trained (akin to self-supervision) by sampling points from the Calabi-Yau space and imposing the constraints entering the theorem as customized loss functions.

  * **Di Luo, IAIFI Fellow**
    * **Friday, October 8, 2:00-3:00pm**
    * *"Machine Learning for Quantum Many-body Physics"*
    * Abstract: The study of quantum many-body physics plays an crucial role across condensed matter physics, high energy physics and quantum information science. Due to the exponential growing nature of Hilbert space, challenges arise for exact classical simulations of high dimensional wave function which is the core object in quantum many-body physics. A natural question comes as whether machine learning, which is powerful for processing high dimensional probability distribution, can provide new methods for studying quantum many-body physics. In contrast to the standard high dimensional probability distribution, the wave function further exhibits complex phase structure and rich symmetries besides high dimensionality. It opens up a series of interesting questions for high dimensional optimization, sampling and representation imposed by quantum many-body physics. In this talk, I will discuss recent advancement of the field and present (1) neural network representations for quantum states with Fermionic anti-symmetry and gauge symmetries; (2) neural network simulations for ground state and real time dynamics in condensed matter physics, high energy physics and quantum information science; (3) quantum control protocol discovery with machine learning.

  * **Cengiz Pehlevan, Assistant Professor, Applied Mathematics, Harvard University (SEAS)**
    * **Friday, October 22, 2:00-3:00pm**
    * *"Inductive bias of neural networks"*
    * Abstract: A learner’s performance depends crucially on how its internal assumptions, or inductive biases, align with the task at hand. I will present a theory that describes the inductive biases of neural networks in the infinite width limit using kernel methods and statistical mechanics. This theory elucidates an inductive bias to explain data with "simple functions" which are identified by solving a related kernel eigenfunction problem on the data distribution. This notion of simplicity allows us to characterize whether a network is compatible with a learning task, facilitating good generalization performance from a small number of training examples. I will present applications of the theory to deep networks (at finite width) trained on synthetic and real datasets, and recordings from the mouse primary visual cortex. Finally, I will briefly present an extension of the theory to out-of-distribution generalization.
    
  * **Bryan Ostdiek, Postdoctoral Fellow, Theoretical Particle Physics, Harvard University**
    * **Friday, November 5, 2:00-3:00pm**
    * *"Lessons from the Dark Machines Anomaly Score Challenge"*
    * Abstract: With LHC experiments producing strong exclusion bounds on theoretical new physics models, there has been recent interest in model agnostic methods to search for physics beyond the standard model. The Dark Machines group conducted a "challenge" as an open playground to examine unsupervised anomaly detection methods on simulated collider events. In this discussion, I briefly motivate and introduce anomaly detection, along with the public data set. We found that the methods which performed best across a wide range of signals shared a common feature; the metric for determining how anomalous an event is depends only on how the event can be encoded into a small representation - there is no decoding step. The discussion will start with speculations about why the "fixed target" encoding can work and look to future tests.

  * **Tess Smidt, Assistant Professor, EECS, MIT**
    * **Friday, November 19, 2:00-3:00pm**
    * *"Unexpected properties of symmetry equivariant neural networks"*
    * Abstract: Physical data and the way that it is represented contains rich context, e.g. symmetries, conserved quantities, and experimental setups. There are many ways to imbue machine learning models with this context (e.g. input representation, training schemes, constraining model structure) and each vary in their flexibility and robustness. In this talk, I’ll give examples of some surprising consequences of what happens when we impose constraints on the functional forms of our models. Specifically, I’ll discuss properties of Euclidean Neural Networks which are constructed to preserve 3D Euclidean symmetry. Perhaps unsurprisingly, symmetry preserving algorithms are extremely data-efficient; they are able to achieve better results with less training data. More unexpectedly, Euclidean Neural Networks also act as “symmetry-compilers”: they can only learn tasks that are symmetrically well-posed and they can also help uncover when there is symmetry implied missing information. I’ll give examples of these properties and how they can be used to craft useful training tasks for physical data. To conclude, I’ll highlight some open questions in symmetry equivariant neural networks particularly relevant to representing physical systems.

  * **Harini Suresh, PhD Student, Computer Science, MIT**
    * **Friday, December 3, 2:00-3:00pm**
    * *"Understanding Sources of Harm throughout the Machine Learning Life Cycle"* 
    * As machine learning increasingly affects people and society, awareness of its potential harmful effects has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it's important that we understand when and how harm might be introduced throughout the ML life cycle. This talk will walk through a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning the data collection, development, and deployment processes.  It will also explore how different sources of harm might motivate different mitigation techniques.  

### Spring 2021

  * **Justin Solomon**
    * **Thursday, February 11, 11am-noon**
    * *"Geometric Data Processing at MIT"*

  * **Phil Harris, Anjali Nambrath, Karna Morey, Michal Szurek, Jade Chongsathapornpong**
    * **Thursday, February 25, 11am-noon**
    * *"Open Data Science in Physics Courses"*

  * **Ge Yang**
    * **Thursday, Mar 11, 11am-noon**
    * *"Learning Task Informed Abstractions"*

  * **Christopher Rackauckas**
    * **Thursday, Mar 25, 11am-noon**
    * *"Overview of SciML"*
    
  * **George Barbastathis/Demba Ba**
    * **Thursday, April 8, 11am-noon**
    * *"On the Countinuum between Dictionaries and Neural Nets for Inverse Problems"*
    
  * **David Kaiser**
    * **Thursday, April 22, 11am-noon**
    * *"Ethics and AI"*
        
  * **Alexander Rakhlin**
    * **Thursday, May 6, 11am-noon**
    * *"Deep Learning: A Statistical Viewpoint"*
        
  * **Edo Berger**
    * **Thursday, May 20, 11am-noon**
    * *"Machine Learning for Cosmic Explosions"*

## Past Journal Clubs

### Fall 2022

* **Kim Nicoli, Grad Student, Technical University of Berlin** 
  * **October 18, 2022, 11:00am-12:00pm**
  * Deep Learning approaches in lattice quantum field theory: recent advances and future challenges**
  * Abstract: Normalizing flows are deep generative models that leverage the change of variable formula to map simple base densities to arbitrary complex target distributions. Recent works have shown the potential of such methods in learning normalized Boltzmann densities in many fields ranging from condensed matter physics to molecular science to lattice field theory. Though sampling from a flow-based density comes with many advantages over standard MCMC sampling, it is known that these methods still suffer from several limitations. In my talk, I will start to give an overview on how to deploy deep generative models to learn Boltzmann densities in the context of a phi^4 lattice field theory. Specifically, I’ll focus on how these methods open up the possibility to estimate thermodynamic observables, i.e., physical observables which depend on the partition function and hence are not straightforward to estimate using standard MCMC methods. In the second part of my talk, I will present two ideas that have been proposed to mitigate the well-known problem of mode-collapse which often occurs when normalizing flows are trained to learn a multimodal target density.  More specifically I’ll talk about a novel “mode-dropping estimator” and path gradients. In the last part of my talk, I’ll present a new idea which aims at using flow-based methods to mitigate the sign problem.

* **Adriana Dropulic, Grad Student, Princeton**
  * **October 4, 2022, 11:00am-12:00pm**
  * *Machine Learning the 6th Dimension: Stellar Radial Velocities from 5D Phase-Space Correlations*
  * Abstract: The Gaia satellite will observe the positions and velocities of over a billion Milky Way stars. In the early data releases, most observed stars do not have complete 6D phase-space information. We demonstrate the ability to infer the missing line-of-sight velocities until more spectroscopic observations become available. We utilize a novel neural network architecture that, after being trained on a subset of data with complete phase-space information, takes in a star's 5D astrometry (angular coordinates, proper motions, and parallax) and outputs a predicted line-of-sight velocity with an associated uncertainty. Working with a mock Gaia catalog, we show that the network can successfully recover the distributions and correlations of each velocity component for stars that fall within ~5 kpc of the Sun. We also demonstrate that the network can accurately reconstruct the velocity distribution of a kinematic substructure in the stellar halo that is spatially uniform, even when it comprises a small fraction of the total star count. We apply the neural network to real Gaia data and discuss how the inferred information augments our understanding of the Milky Way's formation history. 

* **Iris Cong, Grad Student, Harvard**
  * **September 27, 2022, 11:00am-12:00pm**
  * *Quantum Convolutional Neural Networks*
  * Abstract: Convolutional neural networks (CNNs) have recently proven successful for many complex applications ranging from image recognition to precision medicine. In the first part of my talk, motivated by recent advances in realizing quantum information processors, I introduce and analyze a quantum circuit-based algorithm inspired by CNNs. Our quantum convolutional neural network (QCNN) uses only O(log(N)) variational parameters for input sizes of N qubits, allowing for its efficient training and implementation on realistic, near-term quantum devices. To explicitly illustrate its capabilities, I show that QCNN can accurately recognize quantum states associated with a one-dimensional symmetry-protected topological phase, with performance surpassing existing approaches. I further demonstrate that QCNN can be used to devise a quantum error correction (QEC) scheme optimized for a given, unknown error model that substantially outperforms known quantum codes of comparable complexity. The design of such error correction codes is particularly important for near-term experiments, whose error models may be different from those addressed by general-purpose QEC schemes. 
If time permits, I will also present our latest results on generalizing the QCNN framework to more accurately and efficiently identify two-dimensional topological phases of matter.

* **Miles Cranmer, Grad Student, Princeton**
  * **September 20, 2022, 11:00am–12:00pm**
  * *Interpretable Machine Learning for Physics*
  * Abstract: Would Kepler have discovered his laws if machine learning had been around in 1609? Or would he have been satisfied with the accuracy of some black box regression model, leaving Newton without the inspiration to find the law of gravitation? In this talk I will present a review of some industry-oriented machine learning algorithms, and discuss a major issue facing their use in the natural sciences: a lack of interpretability. I will then outline several approaches I have created with collaborators to help address these problems, based largely on a mix of structured deep learning and symbolic methods. This will include an introduction to the PySR software (https://astroautomata.com/PySR), a Python/Julia package for high-performance symbolic regression. I will conclude by demonstrating applications of such techniques and how we may gain new insights from such results.
  * Resources: [https://arxiv.org/abs/2207.12409](https://arxiv.org/abs/2207.12409); [https://arxiv.org/abs/2202.02306](https://arxiv.org/abs/2202.02306); [https://arxiv.org/abs/2006.11287](https://arxiv.org/abs/2006.11287)

* **Anindita Maiti, Grad Student, Northeastern**
  * **September 13, 2022, 11:00am-12:00pm**
  * *A Study of Neural Network Field Theories*
  * Abstract: I will present a systematic exploration of field theories arising in Neural Networks, using a dual framework given by Neural Network parameters. The infinite width limit of NN architectures, combined with i.i.d. parameters, lead to Gaussian Processes in Neural Networks by the Central Limit Theorem (CLT), corresponding to generalized free field theories. Small and large violations of the CLT respectively lead to weakly coupled and non-perturbative non-Lagrangian field theories in Neural Networks. Non-Gaussianity, locality (via cluster decomposition), and symmetries of Neural Network field theories are examined via NN parameter space, without necessitating the knowledge of field theoretic actions. Thus, Neural Network field theories, in conjunction to this duality via parameters, may have potential implications for Physics and Machine Learning both.
  * Resources: [https://arxiv.org/abs/2106.00694](https://arxiv.org/abs/2106.00694)

### Spring 2022
* **Jessie Micallef, PhD Student, Michigan State University & Incoming IAIFI Fellow**
  * **March 10, 2022, 11:00am-12:00pm**
  * *"Adapting CNNs to Reconstruct Sparse, GeV-Scale IceCube Neutrino Events"*
  * Resources:
    * [Reconstructing Neutrino Energy using CNNs for GeV Scale IceCube Events](https://pos.sissa.it/395/1053/pdf)
    * [Direction Reconstruction using a CNN for GeV-Scale Neutrinos in IceCube](https://pos.sissa.it/395/1054/pdf)
* **Denis Boyda, Postdoctoral Appointee, Argonne National Laboratory & Incoming IAIFI Fellow**
  * **RESCHEDULED: March 17, 2022, 11:00am-12:00pm**
  * *"Overview of some popular Machine Learning frameworks for data parallelism"*
  * Resources: 
    * [S. Li et. al. PyTorch Distributed: Experiences on Accelerating Data Parallel Training. 2020.](https://arxiv.org/abs/2006.15704) arXiv:2006.15704
    * [A. Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep learning in TensorFlow. 2018.](https://arxiv.org/abs/1802.05799) arXiv:1802.05799
    * [S. Rajbhandari et.al. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. 2020.](https://arxiv.org/abs/1910.02054) arXiv:1910.02054​
* **Yin Lin, Postdoctoral Researcher, MIT**
  * **April 7, 2022, 11:00am-12:00pm**
  * *"Accelerating Dirac equation solves in lattice QFT with neural-network preconditioners"*
  * Resources:
    * [An Introduction to the Conjugate Gradient Method Without the Agonizing Pain](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf)
    * [Iterative Methods for Sparse Linear Systems](https://www-users.cse.umn.edu/~saad/IterMethBook_2ndEd.pdf)
    * [Deep Learning of Preconditioners for Conjugate Gradient Solvers in Urban Water Related Problems](https://arxiv.org/abs/1906.06925)
    * [Learning to Optimize Non-Rigid Tracking](https://arxiv.org/abs/2003.12230)
* **Anatoly Dymarsky, Associate Professor, University of Kentucky**
  * **April 14, 2022, 11:00am-12:00pm**
  * *Tensor network to learn the wave function of data*
  * Abstract: We use tensor network-based architecture to train a network which simultaneously accomplishes two tasks: image classification and image sampling. We argue that simultaneous performance of these tasks means our network has successfully learned the whole "manifold of data" (using the terminology from the literature) - namely all possible images of a particular kind. We use a black and white version of MNIST, hence our network learns all possible images depicting a particular digit. We access global properties of the "manifold of data" by calculating its size. Thus, we found there are 2^72 possible images of digit 3. We explain this number is robust and largely independent of the details of training process etc. 
  * Resources: 
    * [Tensor network to learn the wavefunction of data](https://arxiv.org/abs/2111.08014)
* **Carolina Cuesta, PhD Student, Durham University & Incoming IAIFI Fellow**
  * **April 21, 2022, 11:00am-12:00pm**
  * *Equivariant normalizing flows and their application to cosmology*
  * Resources:
    * [https://arxiv.org/abs/2202.05282](https://arxiv.org/abs/2202.05282)
    * [https://arxiv.org/abs/2105.09016](https://arxiv.org/abs/2105.09016)
* **Benjamin Fuks, Professor, Sorbonne University**
  * **April 28, 2022, 11:00am-12:00pm**
  * *Precision simulations for new physics*
  * Resources:
    * [https://arxiv.org/abs/1907.04898](Precision simulations for new physics (JHEP 12 (2019) 008))
    * [https://arxiv.org/abs/1901.09937](How precision allows us to design new variables to look for signals (Phys. Rev. D 100, 074010 (2019))
    * [https://arxiv.org/abs/2109.11815](Trying to do better with boosted decision trees on the basis of tree-level simulations  (JHEP 04 (2022) 015))
* **Dylan Hadfield, Assistant Professor, MIT** 
  * **May 5, 2022, 11:00am-12:00pm**
  * *Overoptimization, Incompleteness, and Goodhart's Law*
  * Resources:
    * [https://arxiv.org/abs/1611.08219](https://arxiv.org/abs/1611.08219)
    * [https://arxiv.org/abs/1705.09990](https://arxiv.org/abs/1705.09990)
    * [https://arxiv.org/abs/2102.03896](https://arxiv.org/abs/2102.03896)
* **Mark Hamilton, Graduate Student, MIT**
  * **May 12, 2022, 11:00am-12:00pm**
  * *Unsupervised Semantic Segmentation by Distilling Feature Correspondences*
  * Resources:
    * [Website](https://mhamilton.net/stego.html)
    * [Paper](https://arxiv.org/abs/2203.08414)
    * [Code](https://aka.ms/stego-code)
* **Manami Kanemura, Undergraduate Student, Northeastern University (completed co-op with Bryan Ostdiek)**
  * **May 26, 2022, 11:00am-12:00pm**
  * *Using Soft-Introspection to improve anomaly detection at LHC*
  * Resources:
    * [Soft-IntroVAE: Analyzing and Improving the Introspective Variational Autoencoder](https://arxiv.org/abs/2012.13253)
    * [Challenges for Unsupervised Anomaly Detection in Particle Physics](https://arxiv.org/abs/2110.06948)

### Fall 2021
  * **Michael Douglas**
    * **Thursday, September 23, 11:00am-12:00pm**
    * *"Solving Combinatorial Problems using AI/ML"*
    * Abstract/Resources: [Bright et al 1907.04408](https://arxiv.org/abs/1907.04408); [Heule et al 1905.10192](https://arxiv.org/abs/1905.10192); [Halverson et al 1903.11616](https://arxiv.org/abs/1903.11616); [McAleer et al 1805.07470](https://arxiv.org/abs/1805.07470); [Gukov et al 2010.16263](https://arxiv.org/abs/2010.16263); General sources on reinforcement learning: [Sutton and Bardo](https://www.davidsilver.uk/teaching), [The MathCheck SAT+CAS system](https://uwaterloo.ca/mathcheck)

  * **Ziming Liu**
    * **Thursday, October 7, 11:00am-12:00pm**
    * *"Dynamics in Modern Deep Learning Models"*
    * Abstract/Resources: [Transient Chaos in BERT](https://arxiv.org/pdf/2106.03181.pdf); [Memory and attention in deep learning](https://arxiv.org/pdf/2107.01390.pdf); [The Brownian motion in the transformer model](https://arxiv.org/pdf/2107.05264.pdf)

  * **Ge Yang**
    * **Thursday, October 21, 11:00am-12:00pm**
    * *"Learning and Generalization: Revisiting Neural Representations"*
    * Abstract/Resources: Understanding how deep neural networks learn and generalize has been a central pursuit of intelligence research. This is because we want to build agents that can learn quickly from a small amount of data, that also generalizes to a wider set of scenarios. In this talk, we take a systems approach by identifying key bottleneck components that limits learning and generalization. We will present two key results — overcoming the simplicity bias of neural value approximation via random Fourier features and going beyond the training distribution via invariance through inference.

  * **Eric Michaud, PhD Student, MIT**
    * **Thursday, November 18, 11:00am-12:00pm**
    * *"Curious Properties of Neural Networks"*
    * Abstract/Resources: In this informal talk/discussion, I will highlight some facts about neural networks which I find to be particularly fun and surprising. Possible topics could include the Lottery Ticket Hypothesis (https://arxiv.org/abs/1803.03635), Double Descent (https://arxiv.org/abs/1912.02292), and “grokking” (https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf). There will be time for discussion and for attendees to bring up their own favorite surprising facts about deep learning.

  * **Murphy Niu, Google Quantum AI**
    * **Thursday, December 3, 11:00am-12:00pm**
    * *"Entangling Quantum Generative Adversarial Networks using Tensorflow Quantum"*
    * Abstract/Resources: [https://arxiv.org/pdf/2105.00080.pdf](https://arxiv.org/pdf/2105.00080.pdf); [https://arxiv.org/pdf/2003.02989.pdf%20-%20Page%202.pdf](https://arxiv.org/pdf/2003.02989.pdf%20-%20Page%202.pdf)

### Spring 2021
  * **Anindita Maiti**
    * **Wednesday, February 17**
    * *"Neural Networks and Quantum Field Theory"*
    * Abstract/Resources: [https://arxiv.org/abs/2008.08601](https://arxiv.org/abs/2008.08601)

  * **Jacob Zavatone-Veth**
    * **Tuesday, March 2**
    * *"Non-Gaussian Processes and Neural Networks at Finite Widths"*
    * Abstract/Resources: [https://arxiv.org/abs/1910.00019](https://arxiv.org/abs/1910.00019)

  * **Di Luo**
    * **Tuesday, April 6**
    * *"Simulating Quantum Many-Body Physics with Neural Network Representation"*
    * Abstract/Resources: [https://arxiv.org/abs/1807.10770](https://arxiv.org/abs/1807.10770); [https://arxiv.org/pdf/1912.11052.pdf](https://arxiv.org/pdf/1912.11052.pdf); [https://arxiv.org/abs/2012.05232](https://arxiv.org/abs/2012.05232)

  * **Anna Golubeva**
    * **Tuesday, April 27**
    * *"Are Wider Nets Better Given the Same Number of Parameters?"*
    * Abstract/Resources: [https://arxiv.org/abs/2010.14495](https://arxiv.org/abs/2010.14495)

  * **Siddharth Mishra-Sharma**
    * **Tuesday, May 11**
    * *Simulation-Based Inference Focusing on Astrophysical Applications*
    * Abstract/Resources: [https://arxiv.org/abs/1911.01429](https://arxiv.org/abs/1911.01429); [https://arxiv.org/abs/1909.02005](https://arxiv.org/abs/1909.02005)

### Fall 2020
  * **Bhairav Mehta**
    * **Tuesday, October 20**
    * *"Learning Invariances"*
    * Abstract/Resources: [https://arxiv.org/abs/2009.00329](https://arxiv.org/abs/2009.00329)

  * **Andrew Tan**
    * **Wednesday, November 4**
    * *"Estimating Mutual Information"*
    * Abstract/Resources: [https://arxiv.org/abs/1905.06922](https://arxiv.org/abs/1905.06922)

  * **Ziming Liu**
    * **Wednesday, November 18**
    * *"Scaling Laws of Learning"*
    * Abstract/Resources: [https://arxiv.org/abs/2010.14701](https://arxiv.org/abs/2010.14701); [https://arxiv.org/abs/2004.10802](https://arxiv.org/abs/2004.10802); [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361)

   * **Dan Roberts**
     * **Wednesday, December 2**
     * *"Effective Theory of Deep Learning"*

## Other Past Internal Events 
### Community Building
  * **Spring 2021 Virtual Networking**
    * **Thursday, May 13, 11:00am-12:00pm**
  * **Summer 2021 Virtual Networking**
    * **Thursday, August 19, 12:00pm-1:30pm**
 * **Fall 2021 Networking (in person)**
    * **Friday, October 29, 5:30pm-7:30pm**

### AI Lightning Talks
  * **Friday, December 17, 2:00-3:00pm**
  * IAIFI researchers from the AI thrust presented their work to IAIFI members with a goal of sparking opportunities for collaboration.
    * *"Equivariant Contrastive Learning,"* presented by Rumen Dangovski
    * *"Sparse Equivariant Convolutions for Neutrino Event Classification,"* presented by Taritree Wongjirad and Tess Smidt
    * *"Can you see the shape of a jet?,"* presented by Akshunna S. Dogra

### Town Halls
  * **Year 2 State of the IAIFI Town Hall**
    * **Friday, September 10, 2:00-3:00pm** 
  * **Year 1 Early Career Town Hall**
    * **Tuesday, June 8, 11:00am-12:00pm**
  * **Year 1 IAIFI Town Hall**
    * **Monday, February 8, 11:00am-12:00pm

### Research Events

  * **IAIFI Fall 2020 Unconference**
    * **Monday, December 14, 2020, 2pm-5pm**
  * **IAIFI Fall 2020 Symposium**
    * **Monday, November 23, 2020, 2pm-5pm**
  * **AI Thrust Meeting*
    * **Thursday, October 7, 1:00pm-2:00pm**
  * **Physics Theory Thrust Meeting*
    * **Tuesday, October 12, 2:30pm-3:30pm**
  * **Physics Experiment Thrust Meeting*
    * **Monday, October 18, 11:00am-12:00pm*