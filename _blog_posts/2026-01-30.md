---
title: "Testing"
author: Clifford Cheung (Caltech), Aurélien Dersy (Harvard/IAIFI), and Matthew D. Schwartz (Harvard/IAIFI)
date: 2026-01-30
tags: [scattering amplitudes, machine learning, transformers]
mathjax: true
---

<div class="highlight-box">
IAIFI researchers show that transformer models can learn to simplify long expressions for particle-interaction probabilities—rediscovering classic results and proposing new compact forms.
</div>


<!--
category: [research-highlights]
-->

When particles collide—whether in the early universe, inside a detector, or in a simulation—physicists predict **probabilities** for different outcomes. Those probabilities are encoded in mathematical objects called *scattering amplitudes*. The same amplitude can be written in many algebraically equivalent ways: sometimes as a long sum of fractions stretching across pages, and sometimes as a short, elegant expression. Finding the *short* version matters: it reveals hidden structure, speeds up calculations, and can make large-scale simulations and data analysis more efficient.

Over the past few decades, theorists discovered that, in the right variables, many amplitudes are astonishingly simple. But getting from a messy expression to the compact form is painstaking work that mixes mathematical identities with physical constraints—and the trail is easy to lose in long expressions.

This project brings modern AI into that workflow. We teach a transformer—the same family of models used for language—to **rewrite algebra**, not prose. Given a long, correct amplitude, the system proposes a simpler one. It only accepts a rewrite if it passes a stringent physics check: plug in independent test numbers and verify the two expressions give exactly the same result. In other words, the AI is *guided* by physics and *verified* by physics.

With this setup, the model rediscovers famous compact results and even suggests new short formulas for certain processes. It’s a concrete example of two-way exchange: **physics gives AI structure** (exact identities that generate training data and define correctness), and **AI gives physics leverage** (automated simplification that exposes patterns and accelerates computation). Think of it as using a language model to edit algebraic expressions—line by line—until the underlying simplicity shines through.

**TL;DR.** We use transformer models to take very long—but correct—physics formulas and automatically rewrite them into much shorter, equivalent ones. The system rediscovers classic compact results and proposes new ones, with each candidate strictly checked against physics.

---

### Why this is exciting

#### Physics → AI
- **Physics structure as supervision.** The method treats amplitudes as a “language” governed by exact identities—Schouten relations, momentum conservation, and little-group/dimensional constraints—so training data can be generated synthetically and verified numerically.
- **Spinor-helicity is the right vocabulary.** On-shell amplitudes are often surprisingly simple (e.g., Parke–Taylor), providing a strong inductive bias: there *exists* a short description to aim for.

#### AI → Physics
- **From hundreds of terms to one.** Starting with lengthy diagram-derived expressions, the pipeline collapses them to compact answers—recovering known on-shell results like Parke–Taylor from scratch.
- **New compact formulas.** For five-point processes with scalars and gravitons, the system finds remarkably short expressions, making analytic structure easier to see and reuse.
- **A practical, checkable tool.** Outputs are exact algebraic expressions validated on independent phase-space points, so they are straightforward to verify and plug into downstream calculations.

---

### What the paper does

![Sequential simplification pipeline: projection to an embedding space, grouping of similar terms, one-shot transformer simplification, and an iterative loop until convergence.](/images/blog-Fig1.png)

*Figure 1: The end-to-end pipeline used in the paper.*

#### 1) One-shot symbolic simplification with a transformer
Input amplitudes $$\mathcal{M}$$ are tokenized (prefix notation), and an encoder–decoder is trained to output a simpler $$\overline{\mathcal{M}}$$. Training pairs come from *backward generation*: start from a compact target and apply known identities a few times to create harder inputs. At inference, beam search proposes candidates that are **accepted only if numerically equivalent**.
<!-- 
![Bar charts showing the fraction of test expressions that simplify to equal or shorter forms as beam size increases, across 4-, 5-, and 6-point cases.](images/blog-Fig2.png)
One-shot simplification performance vs beam size.
 -->
#### 2) Contrastive embeddings for *which* terms to combine
Long expressions can have hundreds of numerator terms, making global attention unwieldy. A **projection transformer** is trained with supervised contrastive loss so that terms participating in the *same identity* (e.g., a Schouten triplet) are close in embedding space. Cosine similarity then gives a reliable heuristic for grouping.

<!-- ![Cosine similarity vs. identity distance (two panels).](images/blog-amplitudes-fig8.png)

*Figure 8: The contrastive embedding reflects algebraic proximity—terms that combine under the same identity lie close together.*
 -->

#### 3) Sequential simplification at scale
Using these embeddings, the algorithm **groups** nearby terms, applies the one-shot simplifier to each group, **substitutes** the result back, and **repeats** with a gradually relaxed similarity cutoff. On real four- and five-point gauge- and gravity-theory amplitudes, this iterative loop achieves near-perfect recovery for modest lengths and substantial shrinkage even for very long inputs.

![Performance on real amplitudes (two panels).](images/blog-amplitudes-fig9.png)

*Without one-shot learning, only expressions with less than around 20 terms can be simplified. With contrastive learning much longer expressions can be simplified*

---

### A taste of the results

<div class="math-callout">
  <div class="title">Parke–Taylor (MHV gluons)</div>
  <div class="eq">
  $$A(1^{+} 2^{+} \cdots i^{-} \cdots j^{-} \cdots n^{+})
    \;=\;
    \frac{\langle i j\rangle^{4}}
         {\langle 1 2\rangle \langle 2 3\rangle \cdots \langle n 1\rangle}\,.$$
  </div>
  <div class="caption">Recovered automatically from long diagram-derived expressions.</div>
</div>
<!-- 
**Recovering Parke–Taylor.** From long tree-level inputs, the system outputs a compact color-ordered MHV expression
$$
A(1^{+}2^{+}\cdots i^{-} \cdots j^{-} \cdots n^{+})
\;=\;
\frac{\langle i j\rangle^4}{\langle 1 2\rangle\,\langle 2 3\rangle\,\cdots\,\langle n 1\rangle}\,.
$$
 -->
**New compact forms with gravity.** For five-point processes involving scalars and same-helicity gravitons, long sums collapse to short closed forms. These concise outputs highlight hidden structure and are simple to verify numerically.

<div class="math-callout">
  <div class="title">Compact mixed amplitude (3 scalars + 2 gravitons)</div>
  <div class="eq">
  $$\mathcal{M}(\phi\phi\phi\,h^{+}h^{+})
    \;=\;
    \frac{\langle 12\rangle \langle 13\rangle \langle 23\rangle}{\langle 24\rangle \langle 25\rangle \langle 45\rangle}
    \left(
      \frac{[14][35]}{\langle 14\rangle \langle 35\rangle}
      -
      \frac{[15][34]}{\langle 15\rangle \langle 34\rangle}
    \right)\,.$$
  </div>
  <div class="caption">Two-term final result obtained by sequential simplification from a 298-term input.</div>
</div>

---

### How it works (at a glance)

1. **Data:** Generate compact targets; scramble with Schouten/momentum-conservation identities (and related “multiply by 1 / add 0” tricks) while enforcing little-group scaling and mass dimension.
2. **One-shot model:** Encoder–decoder transformer maps tokenized inputs to simplified outputs; candidates are kept only if numerically equivalent.
3. **Term embeddings:** A projection transformer trained with supervised contrastive loss clusters identity-related terms; cosine similarity guides grouping.
4. **Iterative loop:** Group → simplify → substitute → repeat with a decaying similarity cutoff until no further progress.

---

### Limitations and outlook

- **Range of one-shot generalization** drops for very long sequences or many identity steps; the sequential method recovers much of this but extremely long inputs still pose challenges without larger beams or additional training.
- **Next steps** could add more identities, allow beneficial detours that temporarily increase length, adopt tree-search strategies, and port the approach to other variable choices (e.g., momentum twistors).

---

### Learn more

[Paper](https://arxiv.org/abs/2408.04720){:.button.button--outline-primary.button--pill.button--sm}
[Code](https://github.com/aureliendersy/spinorhelicity){:.button.button--outline-primary.button--pill.button--sm}
[More](https://spinorhelicity.streamlit.app){:.button.button--outline-primary.button--pill.button--sm}

*This work was supported in part by the National Science Foundation via the NSF AI Institute for Artificial Intelligence and Fundamental Interactions (IAIFI).*

---

---
